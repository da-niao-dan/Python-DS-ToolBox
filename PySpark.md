# <a name="PySpark">PySpark</a> 
When to work with Spark?
Normally we just use pandas. However, pandas gets extremely slow when the datasets becomes larger.

Before jumping to Spark, be clear about your purpose. If you just want to process several files with ~2GB size, and that they can be processed line by line without further calculations, you may want to go for line by line approach, because pandas becomes slow when Input-Output flow is large.

If you need to perform more calculations or the size of tables gets to TB size or larger size, you may want to consider to use Spark!

## First Impression of Spark

Spark is a technology for parellel computing on clusters. I would like to think of it as pandas on clusters (very inaccurate analogy).
Resilient Distributed Dataset is the basic building blocks in Spark. DataFrame is built upon RDD with built in optimizations when it comes to table operations. I would like to think of it as DataFame in pandas (again, just an analogy).

PySpark reference is [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html).

To use spark in Python, you need to instantiate a SparkContext object in python. You can think of it as a connection to your control server of your cluster.
Then you need to create a SparkSession, of which you can think as an interface to this connection.

```python
## Assume we have loaded a SparkSession called spark
## Here we are going to practice how to create a SparkSession
# Import SparkSession from pyspark.sql
from pyspark.sql import SparkSession

# Create my_spark
my_spark = SparkSession.builder.getOrCreate()

# Print
print(my_spark)
```

From now on let's assume spark stands for SparkSession NOT A SparkContext anymore. Just a naming stuff.
Your SparkSession has an attribute called catalog with lists of all data inside the session on your cluster. There are several methods to get information.
For example:

```python
# Print the tables in the catalog
print(spark.catalog.listTables())
```

Amazingly, you can quey tables in spark sessions like datasets in sql databases.

```python
# Assume a table of flights is shown in your catalog
query = "FROM flights SELECT * LIMIT 10"

# Get the first 10 rows of flights, flights10 will be a DataFrame
flights10 = spark.sql(query)

# Show the results
flights10.show()
```

You can convert spark DataFrames to pandas DataFrames and work on it locally:

```python
# Example query
query = "SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"

# Run the query
flight_counts = spark.sql(query)

# Convert the results to a pandas DataFrame
pd_counts = flight_counts.toPandas()

# Print the head of pd_counts
print(pd_counts.head())
```

Convert a pandas DataFrame to a spark DataFrame and (not automatically but after some action) work on clusters!

```python
# Create pd_temp
pd_temp = pd.DataFrame(np.random.random(10))

# Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

# Examine the tables in the catalog
print(spark.catalog.listTables())

# Add spark_temp to the catalog, register it using name "temp", createOrReplaceTempView garantees your table names are not duplicates, it will update existing table if exists.
spark_temp.createOrReplaceTempView("temp")

# Examine the tables in the catalog again
print(spark.catalog.listTables())
```

Without working with pandas, let's load the data directly.

```python
# Take an example
file_path = "/usr/local/share/datasets/airports.csv"

# Read in the airports data
airports = spark.read.csv(file_path,header=True)

# Show the data
airports.show()
```

## Manipulating Data

From now on, we have to recognize the unique functionalities of Spark. Forget about pandas dataframes, because those intuitions are not helpful anymore.

Column is an Object type in Spark, and it can be created by `df.colName`.

Unlike pandas DataFrame, Spark DataFrame is immutable, meaning you cannot change columns in place.

To add new column generated by some operation on old column to a df you do something like this: `df=df.withColumn("newCol", df.oldCol + 1)`.
To replace an old column you do this:`df=df.withColumn("oldCol", df.oldCol + 1)`.

```python
# Create the DataFrame flights
flights = spark.table("flights")

# Show the head
flights.show()

# Add duration_hrs
flights = flights.withColumn("duration_hrs", flights.air_time/60.)

# Rename column A to B
flights = flights.withColumnRenamed("A","B")
```

### Filtering Data

Passing string of SQL code or Booleans are the same.

```python
# Filter flights by passing a string
long_flights1 = flights.filter("distance > 1000")

# Filter flights by passing a column of boolean values
long_flights2 = flights.filter(flights.distance > 1000)

# Print the data to check they're equal
long_flights1.show()
long_flights2.show()
```

You may consider filter is like 'SELECT * FROM table, WHERE(your SQL filtering condition here)'

### Selecting

Recall the 'SELECT' statement in SQL.
The 'select' in DataFrame is even more powerful.
'selectExpr' is basically equivalent to 'select' but taking string SQL code as argument.

```python
# Select the first set of columns
selected1 = flights.select('tailnum','origin','dest')

# Select the second set of columns
temp = flights.select(flights.origin, flights.dest, flights.carrier)

# Define first filter
filterA = flights.origin == "SEA"

# Define second filter
filterB = flights.dest == "PDX"

# Filter the data, first by filterA then by filterB
selected2 = temp.filter(filterA).filter(filterB)

# Define avg_speed
avg_speed = (flights.distance/(flights.air_time/60)).alias("avg_speed")

# Select the correct columns
speed1 = flights.select("origin", "dest", "tailnum", avg_speed)

# Create the same table using a SQL expression
speed2 = flights.selectExpr("origin", "dest", "tailnum", "distance/(air_time/60) as avg_speed")
```

### Aggregating

Aggregating means summerizing a group of data in some sense. like min(), max(), count().

The .groupBy() method of the DataFrame creates an object of type pyspark.sql.GroupedData. Passing arguments to .groupby() is similar to using groupby in SQL.

```python
# Find the shortest flight from PDX in terms of distance
flights.filter(flights.origin=='PDX').groupBy().min('distance').show()

# Find the longest flight from SEA in terms of air time
flights.filter(flights.origin=='SEA').groupBy().max('air_time').show()

# Average duration of Delta flights
flights.filter(flights.carrier=="DL").filter(flights.origin=="SEA").groupBy().avg('air_time').show()

# Total hours in the air
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum("duration_hrs").show()

# Group by tailnum
by_plane = flights.groupBy("tailnum")

# Number of flights each plane made
by_plane.count().show()

# Group by origin
by_origin = flights.groupBy("origin")

# Average duration of flights from PDX and SEA
by_origin.avg("air_time").show()
```

GroupData objects have another useful method .agg(), which allows you to pass an aggregat column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.
This submodule has many useful functions.

Example:

```python
# Import pyspark.sql.functions as F
import pyspark.sql.functions as F

# Group by month and dest
by_month_dest = flights.groupBy('month','dest')

# Average departure delay by month and destination
by_month_dest.avg("dep_delay").show()

# Standard deviation of departure delay
by_month_dest.agg(F.stddev("dep_delay")).show()
```

### Joining

```python
# Examine the data
print(airports.show())

# Rename the faa column
airports = airports.withColumnRenamed("faa","dest")

# Join the DataFrames
flights_with_airports = flights.join(airports,'dest',how='leftouter')

# Examine the new DataFrame
print(flights_with_airports.show())
```

## Machine Learning Pipelines

Before we get to the pipeline, you should understand that there are two main classes for spark: Estimators and Transformers. Estimators have method .fit() and return models. Transformers have method .transform and return DataFrames. Spark modelling mainly relies on these two classes.

First, preprocessing data. Spark requires numerical data for modeling.

```python
# Rename year column
planes = planes.withColumnRenamed("year", "plane_year")

# Join the DataFrames
model_data = flights.join(planes, on='tailnum', how="leftouter")

# Cast the columns to integers
model_data = model_data.withColumn("arr_delay", model_data.arr_delay.cast('integer'))
model_data = model_data.withColumn("air_time", model_data.air_time.cast('integer'))
model_data = model_data.withColumn("month", model_data.month.cast('integer'))
model_data = model_data.withColumn("plane_year", model_data.plane_year.cast('integer'))

# Create the column plane_age
model_data = model_data.withColumn("plane_age", model_data.year - model_data.plane_year)

#Create is_late
model_data = model_data.withColumn("is_late", model_data.arr_delay > 0)

# Convert to an integer
model_data = model_data.withColumn("label", model_data.is_late.cast('integer'))

# Remove missing values
model_data = model_data.filter("arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL")
```

Next, process categorical data.

```python
# Create a StringIndexer
carr_indexer = StringIndexer(inputCol="carrier", outputCol="carrier_index")

# Create a OneHotEncoder
carr_encoder = OneHotEncoder(inputCol="carrier_index", outputCol="carrier_fact")

# Create a StringIndexer
dest_indexer = StringIndexer(inputCol="dest", outputCol="dest_index")

# Create a OneHotEncoder
dest_encoder = OneHotEncoder(inputCol="dest_index", outputCol="dest_fact")
```

The next step is to combine all of the columns of features to a single column.

```python
# Make a VectorAssembler
vec_assembler = VectorAssembler(inputCols=["month", "air_time", "carrier_fact", "dest_fact", "plane_age"], outputCol="features")
```

Next, use Pipeline to combine all Transformers and Estimators.

```python
# Import Pipeline
from pyspark.ml import Pipeline

# Make the pipeline
flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])
```

Then we are going to split the data, *after* these transformations. Operations like StringIndexer don't always give the same index even with the same list of strings.

```python
# Fit and transform the data
piped_data = flights_pipe.fit(model_data).transform(model_data)

# Split the data into training and test sets
training, test = piped_data.randomSplit([.6, .4])
```

OK, finally the fun part!

```python
# Import LogisticRegression
from pyspark.ml.classification import LogisticRegression

# Create a LogisticRegression Estimator
lr = LogisticRegression()

# Import the evaluation submodule
import pyspark.ml.evaluation as evals

# Create a BinaryClassificationEvaluator
evaluator = evals.BinaryClassificationEvaluator(metricName="areaUnderROC")

# Import the tuning submodule
import pyspark.ml.tuning as tune

# Create the parameter grid
grid = tune.ParamGridBuilder()

# Add the hyperparameter
grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01)) ## model
grid = grid.addGrid(lr.elasticNetParam, [0 , 1]) ## regularization

# Build the grid
grid = grid.build()

# Create the CrossValidator
cv = tune.CrossValidator(estimator=lr,
               estimatorParamMaps=grid,
               evaluator=evaluator
               )

# Fit cross validation models
models = cv.fit(training)

# Extract the best model
best_lr = models.bestModel

# Use the model to predict the test set
test_results = best_lr.transform(test)

# Evaluate the predictions
print(evaluator.evaluate(test_results))
```

### Details about Creating a SparkSession

```python
# Import the PySpark module
from pyspark.sql import SparkSession

# Create SparkSession object
spark = SparkSession.builder \
                    .master('local[*]') \ ## To connect to a remote location, use: spark://<IP address | DNS name>:<port>
                    .appName('test') \
                    .getOrCreate()

# What version of Spark?
print(spark.version)

# Terminate the cluster
spark.stop()
```

### Details about loading data into Spark

DataFrame:
Select Methods:

* count()
* show()
* printSchema()

Selected attributes:

* dtypes

Reading data from csv: `cars = spark.read.csv("cars.csv",header=True)`
Optional arguments:

* header
* sep
* schema - explicit column data types
* inferSchema - deduce column data types?
* nullValue -placeholder for missing data

This action can have problems. We may prefer `cars = spark.read.csv("cars.csv", header=True, inferSchema=True, nullValue='NA')` (Always good to explicitly define missing values.)

check `cars.dtypes` to show datatypes of each column. It turns out, columns with missing data will have 'NA' string, that column will be wrongly interpretated as String column.

In that case, we need to specify the schema by hand.

```python
# Read data from CSV file
flights = spark.read.csv('flights.csv',
                         sep=',',
                         header=True,
                         inferSchema=True,
                         nullValue='NA')

# Get number of records
print("The data contain %d records." % flights.count())

# View the first five records
flights.show(5)

# Check column data types
flights.dtypes

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Specify column names and types
schema = StructType([
    StructField("id", IntegerType()),
    StructField("text", StringType()),
    StructField("label", IntegerType())
])

# Load data from a delimited file
sms = spark.read.csv('sms.csv', sep=';', header=False, schema=schema)

# Print schema of DataFrame
sms.printSchema()
```

### Details about manipulating data

Data selection

```python

# Either drop the columns you don't want
cars = cars.drop('maker','model')

# ... or select the columns you do want
cars = cars.select("origin", 'type', 'cyl')

# Filtering out missing vals
## count
cars.filter('cyl is NULL').count()

## drop records with missing values in the cylinders column
cars = cars.filter('cyl IS NOT NULL')

## drop records with any missing data
cars = cars.dropna()
```

Index categorical data.

```python
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(inputCol='type',outputCol='type_idx')

# Assign index values to strings
indexer = StringIndexer(inputCol= 'type',
                        outputCol='type_idx')

# Assign index values to strings
indexer = indexer.fit(cars)

# Create column with index values
cars = indexer.transform(cars)
```

By defaults the most frequent string will get index 0, and the  least frequent string will get the maximum index.

Use `stringOrderType` to change order.

